{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\venkata\n",
      "[nltk_data]     pavan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\venkata pavan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "\n",
    "import os\n",
    "import emoji\n",
    "import time\n",
    "import hashlib\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from math import exp\n",
    "\n",
    "import numpy as np\n",
    "from numpy import sign\n",
    "import ftfy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import ftfy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Conv1D, Dense, Input, LSTM, Embedding, Dropout, Activation, MaxPooling1D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.metrics import  classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "from twitterscraper.query import query_tweets, query_tweets_from_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "\n",
    "Depression_Tweets = 5000  # number of rows to read from DEPRESSIVE_TWEETS_CSV\n",
    "Random_Tweets = 20000 # number of rows to read from RANDOM_TWEETS_CSV\n",
    "Max_Tweet_Length = 280 # Max tweet size\n",
    "Max_Unique_Words = 20000\n",
    "Embedding_Dimension = 300\n",
    "Train = 0.6\n",
    "Test = 0.2\n",
    "Learning_Rate = 0.1\n",
    "EPOCHS= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Depression_Tweets_File = 'Depression_Tweets.csv'\n",
    "Random_Tweets_File = 'Sentiment Analysis Dataset 2.csv'\n",
    "EMBEDDING_FILE = 'GoogleNews-vectors-negative300.bin.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "depressive_tweets_Data = pd.read_csv(Depression_Tweets_File, encoding = \"ISO-8859-1\", usecols = range(0,5), nrows =  Depression_Tweets)\n",
    "random_tweets_Data = pd.read_csv(Random_Tweets_File, encoding = \"ISO-8859-1\", usecols = range(0,4), nrows = Random_Tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>timezone</th>\n",
       "      <th>username</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17/04/2019</td>\n",
       "      <td>18:46:06</td>\n",
       "      <td>Eastern Daylight Time</td>\n",
       "      <td>hilaryjhendel</td>\n",
       "      <td>âThe problem is no one can see my scars.â ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17/04/2019</td>\n",
       "      <td>18:46:05</td>\n",
       "      <td>Eastern Daylight Time</td>\n",
       "      <td>chronicgamerguy</td>\n",
       "      <td>And due to what's going to happen later. My st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17/04/2019</td>\n",
       "      <td>18:45:59</td>\n",
       "      <td>Eastern Daylight Time</td>\n",
       "      <td>jennnaglavin</td>\n",
       "      <td>My doctor continuously tells me not to drink c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17/04/2019</td>\n",
       "      <td>18:45:49</td>\n",
       "      <td>Eastern Daylight Time</td>\n",
       "      <td>amelia_kathleen</td>\n",
       "      <td>âAnXIetY IsNT ReALâ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17/04/2019</td>\n",
       "      <td>18:45:46</td>\n",
       "      <td>Eastern Daylight Time</td>\n",
       "      <td>moharris92</td>\n",
       "      <td>Itâs been so long since Iâve watched any T...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date      time               timezone         username  \\\n",
       "0  17/04/2019  18:46:06  Eastern Daylight Time    hilaryjhendel   \n",
       "1  17/04/2019  18:46:05  Eastern Daylight Time  chronicgamerguy   \n",
       "2  17/04/2019  18:45:59  Eastern Daylight Time     jennnaglavin   \n",
       "3  17/04/2019  18:45:49  Eastern Daylight Time  amelia_kathleen   \n",
       "4  17/04/2019  18:45:46  Eastern Daylight Time       moharris92   \n",
       "\n",
       "                                               tweet  \n",
       "0  âThe problem is no one can see my scars.â ...  \n",
       "1  And due to what's going to happen later. My st...  \n",
       "2  My doctor continuously tells me not to drink c...  \n",
       "3                            âAnXIetY IsNT ReALâ  \n",
       "4  Itâs been so long since Iâve watched any T...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depressive_tweets_Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentSource</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ItemID  Sentiment SentimentSource  \\\n",
       "0       1          0    Sentiment140   \n",
       "1       2          0    Sentiment140   \n",
       "2       3          1    Sentiment140   \n",
       "3       4          0    Sentiment140   \n",
       "4       5          0    Sentiment140   \n",
       "\n",
       "                                       SentimentText  \n",
       "0                       is so sad for my APL frie...  \n",
       "1                     I missed the New Moon trail...  \n",
       "2                            omg its already 7:30 :O  \n",
       "3            .. Omgaga. Im sooo  im gunna CRy. I'...  \n",
       "4           i think mi bf is cheating on me!!!   ...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_tweets_Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#import en_core_web_sm\n",
    "contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\",\n",
    "                    \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \n",
    "                    \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\",\n",
    "                    \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\n",
    "                    \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \n",
    "                    \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\",\n",
    "                    \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n",
    "                    \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\",\n",
    "                    \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \n",
    "                    \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\"today's\": \"today is\",\"tomorrow's\":\"tomorrow is\", \"wasn't\": \"was not\",\n",
    "                    \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\",\n",
    "                    \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \n",
    "                    \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \n",
    "                    \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \n",
    "                    \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\",\"&amp;\":\"and\",\"&lt;\":\"<\",\"&gt;\":\">\",\"&le;\":\"=<\",\"&ge;\":\">=\"}\n",
    "stop_words=set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def removestopwords(line):\n",
    "    words=word_tokenize(line)\n",
    "    wordslist=[]\n",
    "    for word in words:\n",
    "            if not word in stop_words:\n",
    "                wordslist.append(word)\n",
    "    return ' '.join(wordslist)           \n",
    "\n",
    "def _get_contractions(contraction_dict):\n",
    "    contraction_re = re.compile('(%s)' % '|'.join(contraction_dict.keys()))\n",
    "    return contraction_dict, contraction_re\n",
    "\n",
    "contractions, contractions_re = _get_contractions(contraction_dict)\n",
    "\n",
    "def replace_contractions(text):\n",
    "    def replace(match):\n",
    "        return contractions[match.group(0)]\n",
    "    return contractions_re.sub(replace, text)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def nltk2wn_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:                    \n",
    "        return None\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))    \n",
    "    wn_tagged = map(lambda x: (x[0], nltk2wn_tag(x[1])), nltk_tagged)\n",
    "\n",
    "    res_words = []\n",
    "    for word, tag in wn_tagged:\n",
    "        if tag is None:                        \n",
    "            res_words.append(word)\n",
    "        else:\n",
    "            res_words.append(lemmatizer.lemmatize(word, tag))\n",
    "\n",
    "    return ' '.join(res_words)\n",
    "\n",
    "def removeUnnecessaryWords(x):\n",
    "    words=x.split()\n",
    "    for word in words:\n",
    "            if not len(word)>1:\n",
    "               words.remove(word) \n",
    "    if len(words)>1:\n",
    "        return ' '.join(words)\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(tweets):\n",
    "    \n",
    "    cleaned_tweets = []\n",
    "    for tweet in tweets:\n",
    "        x=tweet.lower()\n",
    "        x=replace_contractions(x)                            \n",
    "        #x=remove_nameentity(x)\n",
    "        #remove urls\n",
    "        x = re.sub(r'(via:)? +https?:\\/\\/.*,', ',', x)\n",
    "        #remove datetime\n",
    "        x=re.sub(r'\\d+:?(\\d+)?( +)?([ap]m):?( +([ce]dt)?)?',' ',x)\n",
    "        #x=re.sub(r'cdt +',' ',x)\n",
    "        x=re.sub(r'\\d{4}-\\d{2}-\\d{2} +\\d{2}:\\d{2}:\\d{2},', '', x)\n",
    "        #remove temperature\n",
    "        x=re.sub(r'\\d+\\/\\d+°?[cf]',' ',x)\n",
    "        #remove numbers\n",
    "        x=re.sub(r'[0-9]*,?','',x)\n",
    "        #remove floating point numbers\n",
    "        x=re.sub(r'-?[0-9]*\\.[0-9]*,?',' ',x)\n",
    "        #remove hashtags and user tags\n",
    "        x=re.sub(r'[#@]( +)?[a-zA-Z0-9.,_:]+','',x)\n",
    "        #remove special characters\n",
    "        x=re.sub(r'[:-?;&)(!\"*%_+$~/\\[\\]]','',x)\n",
    "        #remove emoji's\n",
    "        x=emoji.get_emoji_regexp().sub(u'', x)\n",
    "        #replace contraction with acutal words                      \n",
    "        x=removestopwords(x)\n",
    "        x=lemmatize_sentence(x)\n",
    "        x=re.sub(r'\\'s','',x)                            \n",
    "        #removing non word characters and extra spaces in sentence\n",
    "        x=re.sub(r'[^\\w]', ' ', x)\n",
    "        x=re.sub(r'[^a-zA-Z0-9]',' ',x)\n",
    "        x=re.sub(r' +',' ',x)\n",
    "        #removing white space characters\n",
    "        x=re.sub(r' +[a-zA-Z] +','',x)\n",
    "        x=x.strip()\n",
    "        #x=' '.join()\n",
    "        x=removeUnnecessaryWords(x)\n",
    "        cleaned_tweets.append(x)\n",
    "\n",
    "    return cleaned_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "depressive_tweets_arr = [x for x in depressive_tweets_df['tweet']]\n",
    "random_tweets_arr = [x for x in random_tweets_df['SentimentText']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\venkata\n",
      "[nltk_data]     pavan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Depressive_Tweets_Cleaned = clean_tweets(depressive_tweets_arr)\n",
    "Random_Tweets_Cleaned = clean_tweets(random_tweets_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                   I missed the New Moon trailer...'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_tweets_arr[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the problem one see scar know intuitively depression anxiety deep seated insecurity wound stem verbal abusewish beaten marta share one occasiond feel legitimate httpswww hilaryjacobshendel comsingle postthe problem with yelling pic twitter comhcevsawrc'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Depressive_Tweets_Cleaned[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras_preprocessing.text.Tokenizer object at 0x0000020A077CC5C0>\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=Max_Unique_Words)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(Depressive_Tweets_Cleaned + Random_Tweets_Cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# saving\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# loading\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "Depressive_Tweets_vectors = tokenizer.texts_to_sequences(Depressive_Tweets_Cleaned)\n",
    "Random_Tweets_vectors = tokenizer.texts_to_sequences(Random_Tweets_Cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 32056 unique tokens\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data_d tensor: (5000, 280)\n",
      "Shape of data_r tensor: (20000, 280)\n"
     ]
    }
   ],
   "source": [
    "data_depressive_tweets = pad_sequences(Depressive_Tweets_vectors, maxlen=Max_Tweet_Length)\n",
    "data_random_tweets = pad_sequences(Random_Tweets_vectors, maxlen=Max_Tweet_Length)\n",
    "print('Shape of data_d tensor:', data_depressive_tweets.shape)\n",
    "print('Shape of data_r tensor:', data_random_tweets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "Frequent_Words = min(Max_Unique_Words, len(word_index))\n",
    "print(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((Frequent_Words, EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    }
   ],
   "source": [
    "print(Frequent_Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (word, index) in word_index.items():\n",
    "    if word in word2vec.vocab and index < Max_Unique_Words:\n",
    "        embedding_matrix[index-1] = word2vec.word_vec(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 1 1 1]\n",
      "[0 0 0 ... 0 0 0]\n",
      "[1536  704 1943 ... 3089 1070 4882]\n"
     ]
    }
   ],
   "source": [
    "labels_d = np.array([1] * Depression_Tweets)\n",
    "labels_r = np.array([0] * Random_Tweets)\n",
    "print(labels_d)\n",
    "print(labels_r)\n",
    "perm_d = np.random.permutation(len(data_depressive_tweets))\n",
    "print(perm_d)\n",
    "\n",
    "idx_train_d = perm_d[:int(len(data_depressive_tweets)*(TRAIN_SPLIT))]\n",
    "idx_test_d = perm_d[int(len(data_depressive_tweets)*(TRAIN_SPLIT)):int(len(data_depressive_tweets)*(TRAIN_SPLIT+TEST_SPLIT))]\n",
    "idx_val_d = perm_d[int(len(data_depressive_tweets)*(TRAIN_SPLIT+TEST_SPLIT)):]\n",
    "\n",
    "perm_r = np.random.permutation(len(data_random_tweets))\n",
    "idx_train_r = perm_r[:int(len(data_random_tweets)*(TRAIN_SPLIT))]\n",
    "idx_test_r = perm_r[int(len(data_random_tweets)*(TRAIN_SPLIT)):int(len(data_random_tweets)*(TRAIN_SPLIT+TEST_SPLIT))]\n",
    "idx_val_r = perm_r[int(len(data_random_tweets)*(TRAIN_SPLIT+TEST_SPLIT)):]\n",
    "\n",
    "# Combine depressive tweets and random tweets arrays\n",
    "data_train = np.concatenate((data_depressive_tweets[idx_train_d], data_random_tweets[idx_train_r]))\n",
    "labels_train = np.concatenate((labels_d[idx_train_d], labels_r[idx_train_r]))\n",
    "data_test = np.concatenate((data_depressive_tweets[idx_test_d], data_random_tweets[idx_test_r]))\n",
    "labels_test = np.concatenate((labels_d[idx_test_d], labels_r[idx_test_r]))\n",
    "data_val = np.concatenate((data_depressive_tweets[idx_val_d], data_random_tweets[idx_val_r]))\n",
    "labels_val = np.concatenate((labels_d[idx_val_d], labels_r[idx_val_r]))\n",
    "\n",
    "# Shuffling\n",
    "perm_train = np.random.permutation(len(data_train))\n",
    "data_train = data_train[perm_train]\n",
    "labels_train = labels_train[perm_train]\n",
    "perm_test = np.random.permutation(len(data_test))\n",
    "data_test = data_test[perm_test]\n",
    "labels_test = labels_test[perm_test]\n",
    "perm_val = np.random.permutation(len(data_val))\n",
    "data_val = data_val[perm_val]\n",
    "labels_val = labels_val[perm_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDING_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\venkata pavan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From c:\\users\\venkata pavan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# Embedded layer\n",
    "model.add(Embedding(20000, 300,weights=[embedding_matrix], \n",
    "                            input_length=Max_Tweet_Length, trainable=False))\n",
    "# Convolutional Layer\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# LSTM Layer\n",
    "model.add(LSTM(300))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 280, 300)          6000000   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 280, 32)           28832     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 140, 32)           0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 140, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 300)               399600    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 301       \n",
      "=================================================================\n",
      "Total params: 6,428,733\n",
      "Trainable params: 428,733\n",
      "Non-trainable params: 6,000,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\venkata pavan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 15000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "15000/15000 [==============================] - 96s 6ms/step - loss: 0.2884 - acc: 0.8907 - val_loss: 0.1821 - val_acc: 0.9436\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.18206, saving model to model_new.hdf5\n",
      "Epoch 2/10\n",
      "15000/15000 [==============================] - 92s 6ms/step - loss: 0.1748 - acc: 0.9406 - val_loss: 0.1625 - val_acc: 0.9470\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.18206 to 0.16250, saving model to model_new.hdf5\n",
      "Epoch 3/10\n",
      "15000/15000 [==============================] - 91s 6ms/step - loss: 0.1471 - acc: 0.9511 - val_loss: 0.1655 - val_acc: 0.9488\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.16250\n",
      "Epoch 4/10\n",
      "15000/15000 [==============================] - 90s 6ms/step - loss: 0.1272 - acc: 0.9563 - val_loss: 0.1764 - val_acc: 0.9462\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.16250\n",
      "Epoch 5/10\n",
      "15000/15000 [==============================] - 91s 6ms/step - loss: 0.1118 - acc: 0.9619 - val_loss: 0.1821 - val_acc: 0.9432\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.16250\n"
     ]
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', patience=3)\n",
    "checkpoint = ModelCheckpoint('model_new.hdf5', \n",
    "                              monitor='val_loss', \n",
    "                              verbose=1, \n",
    "                              save_best_only=True, \n",
    "                              mode='min')\n",
    "hist = model.fit(data_train, labels_train, \\\n",
    "        validation_data=(data_val, labels_val), \\\n",
    "        epochs=EPOCHS, batch_size=40, shuffle=True, \\\n",
    "        callbacks=[checkpoint, early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 94.64%\n"
     ]
    }
   ],
   "source": [
    "labels_pred = model.predict(data_test)\n",
    "labels_pred = np.round(labels_pred.flatten())\n",
    "accuracy = accuracy_score(labels_test, labels_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Got 63 tweets from username @liz_ngx\n"
     ]
    }
   ],
   "source": [
    "list_of_tweets = query_tweets_from_user(\"@liz_ngx\")\n",
    "\n",
    "tweetsOfUser = [];\n",
    "\n",
    "for tw in list_of_tweets:\n",
    "    tweetsOfUser.append(tw.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cleaned_User_Tweets = clean_tweets(tweetsOfUser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['post concert depression really hit hard twitter comojqbojpy',\n",
       " 'rosie angelll felt like dyinggg twitter comtlhtl',\n",
       " 'jennie cuteee omgggg twitter commdaydgs',\n",
       " 'jisoo grab phone wtf poor baby get scared twitter comwtpoaetx',\n",
       " 'omygod harry style really blackpink concert wtf wtf twitter comrmxswixruj',\n",
       " 'need unprovoked attackhttpstwitter comcutecherrybitchstatus',\n",
       " 'quiet public space stranger stomachpic twitter comnsdxdixegu',\n",
       " 'wow may hardest coat ever see httpstwitter comthabostatus',\n",
       " 'pic twitter comsxwfpqgyh',\n",
       " 'make coachella look like blackpink concertpic twitter comujunrzjs',\n",
       " 'pic twitter comwhhjoitgj',\n",
       " 'pic twitter compjixqc',\n",
       " 'yr ago today single thing could ever dopic twitter comyswekoqsjb',\n",
       " 'oh gosh videoing baby bad dream watch sister pic twitter comacerrik',\n",
       " 'saw adult german shepard diagnose dwarfism today cut thing saw yearpic twitter comfjijaaxwc',\n",
       " 'bruh dog face yo lmfaoooo pic twitter comvqqmcfgj',\n",
       " 'hummingbird drink nectar slow motion accidentally slap hell bee wingpic twitter comqioegkfsp',\n",
       " 'highlight ktl performance tonight phewpic twitter comomkrnto',\n",
       " 'us come south korea know expect obviously guys us totally different world tonight think learn deeply music bring us one ros pic twitter comtucgokp',\n",
       " 'ros skinny eats cd breakfast lunch dinner pic twitter comysnkbpmcpu',\n",
       " 'hardworking talented bitch industry solo song yetpic twitter comrqjwwldka',\n",
       " 'popcorn microwavepic twitter comvanfngtz',\n",
       " 'think siren break sound imgurpic twitter comjymptqtsb',\n",
       " 'past tense william shakespeare would wouldiwas shookspearedhttpstwitter commalenapeaksstatus',\n",
       " 'dog funny cutepic twitter comlzcxwxontp',\n",
       " 'pick llama httpstwitter comgnumanstatus',\n",
       " 'sam say nopepic twitter comipqjkxx',\n",
       " 'twitter comikyhyvraba',\n",
       " 'kid mexico ruthless lmfaopic twitter comvpnzhtemlb',\n",
       " 'pls judgement try bestpic twitter comhklvbemn',\n",
       " 'excellencepic twitter comepefsist',\n",
       " 'want haspic twitter comhbzdawnl',\n",
       " 'peek boopic twitter comdkliusfkr',\n",
       " 'theory plate car driving say ego look like try crash could symbolize crash ego perception weak stop cover eye end relationship twitter comtdcqlsemc',\n",
       " 'twitter sorry maximum gif file size limit mb mb rise gifpic twitter comxvklobsr',\n",
       " 'chip head as pic twitter comhstorvkvj',\n",
       " 'pic twitter comnhjlzmojgb',\n",
       " 'kill love broken record fast music video history surpass like youtube hour min',\n",
       " 'looks good twitter comneizjps',\n",
       " 'brooo emoji challenge group chat cryinggg watch til end pic twitter comwurzlibt',\n",
       " 'mom clutch purse white woman pass grocery store mom stole people baby think steal pursepic twitter comtoryllet',\n",
       " 'stop use plastic ghost get catch save ghostshttpstwitter comflicteriastatus',\n",
       " 'download screaming game omgpic twitter comxbunaig',\n",
       " 'wheezingpic twitter comolnrqkdfkh',\n",
       " 'morning dog heard fridge open get bed come look snack find anything grabbed blanket fall go back bed pic twitter comzdimjrarn',\n",
       " 'even close fuck lid httpstwitter compeacharustatus',\n",
       " 'girl clingy really need lot attention also girls pay attention secondspic twitter combiargkesv',\n",
       " 'jhene aiko make soulful diss track ever pic twitter comxqmupumkgu',\n",
       " 'please begging boob',\n",
       " 'shit still funny tiktok djskskpic twitter comschjeebane',\n",
       " 'nigga korean httpstwitter comdrunksznstatus',\n",
       " 'dog straight teeth ever seenpic twitter comdegkkwxkhr',\n",
       " 'lmfao moe tear real pic twitter comrckloafge',\n",
       " 'pic twitter comhckgnjf',\n",
       " 'look mirror eat drink weekendpic twitter comhtrrrafh',\n",
       " 'almost kill boyfriend slice cheesepic twitter combkmurqogz',\n",
       " 'yo bring fuck cow back httpstwitter comsoionisstatus',\n",
       " 'bitch get movespic twitter comiytdjqwtb',\n",
       " 'say sksksksks meanpic twitter comeafyeytok',\n",
       " 'try cheese challenge baby slick pic twitter comnwablvaaqr',\n",
       " 'get town make pit stop surprise little brother pic twitter comuupurnmgqs',\n",
       " 'hyunadawn way double date jennie kai pic twitter compmszrmjasb',\n",
       " 'retweet support jennie kai relationship retweet hope never find happiness pic twitter comwfkrczn']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Cleaned_User_Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_user = tokenizer.texts_to_sequences(Cleaned_User_Tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[234, 560, 174, 29, 265, 185, 13],\n",
       " [5305, 548, 5, 13],\n",
       " [19853, 13],\n",
       " [2266, 128, 478, 266, 116, 1, 1690, 13],\n",
       " [883, 923, 29, 560, 478, 478, 13],\n",
       " [25, 11975],\n",
       " [1201, 944, 767, 1685, 13],\n",
       " [314, 175, 8196, 9541, 82, 21, 69],\n",
       " [37, 13],\n",
       " [9, 3806, 40, 5, 13],\n",
       " [37, 13],\n",
       " [37, 13],\n",
       " [846, 373, 20, 454, 42, 55, 82, 13],\n",
       " [45, 1137, 116, 38, 282, 46, 393, 37, 13],\n",
       " [325, 1471, 3604, 1147, 20, 456, 42, 325, 13],\n",
       " [3655, 340, 197, 539, 37, 13],\n",
       " [395, 18009, 598, 3317, 2958, 3472, 418, 2515, 13],\n",
       " [3443, 1422, 78, 13],\n",
       " [67,\n",
       "  32,\n",
       "  1383,\n",
       "  5884,\n",
       "  16,\n",
       "  620,\n",
       "  1788,\n",
       "  2652,\n",
       "  67,\n",
       "  484,\n",
       "  572,\n",
       "  141,\n",
       "  78,\n",
       "  19,\n",
       "  315,\n",
       "  5491,\n",
       "  161,\n",
       "  339,\n",
       "  67,\n",
       "  15,\n",
       "  37,\n",
       "  13],\n",
       " [2286, 871, 729, 433, 446, 37, 13],\n",
       " [2282, 392, 4456, 1101, 88, 13],\n",
       " [3121, 13],\n",
       " [19, 18579, 143, 195, 13],\n",
       " [537, 6965, 3086, 17419, 24],\n",
       " [340, 218, 13],\n",
       " [405, 3367, 69],\n",
       " [1173, 27, 13],\n",
       " [13],\n",
       " [194, 2261, 13],\n",
       " [860, 9553, 60, 13],\n",
       " [13],\n",
       " [11, 13],\n",
       " [4182, 13],\n",
       " [4209,\n",
       "  4282,\n",
       "  269,\n",
       "  2231,\n",
       "  27,\n",
       "  2471,\n",
       "  40,\n",
       "  5,\n",
       "  60,\n",
       "  497,\n",
       "  55,\n",
       "  497,\n",
       "  2471,\n",
       "  10737,\n",
       "  1255,\n",
       "  137,\n",
       "  643,\n",
       "  295,\n",
       "  153,\n",
       "  982,\n",
       "  13],\n",
       " [13, 112, 4332, 5409, 1375, 1150, 1248, 4750, 4750, 855, 13],\n",
       " [1638, 127, 226, 37, 13],\n",
       " [37, 13],\n",
       " [310, 8, 823, 579, 661, 161, 228, 906, 5, 428, 104, 488],\n",
       " [15912, 7, 13],\n",
       " [11974, 1606, 492, 1302, 46, 463, 153, 37, 13],\n",
       " [166,\n",
       "  19737,\n",
       "  4268,\n",
       "  449,\n",
       "  374,\n",
       "  592,\n",
       "  2309,\n",
       "  451,\n",
       "  166,\n",
       "  2549,\n",
       "  36,\n",
       "  116,\n",
       "  19,\n",
       "  1425,\n",
       "  13],\n",
       " [137, 100, 3429, 3979, 1, 408, 420],\n",
       " [824, 132, 13],\n",
       " [13],\n",
       " [91,\n",
       "  340,\n",
       "  669,\n",
       "  4044,\n",
       "  343,\n",
       "  1,\n",
       "  115,\n",
       "  32,\n",
       "  40,\n",
       "  2079,\n",
       "  76,\n",
       "  237,\n",
       "  3303,\n",
       "  267,\n",
       "  2,\n",
       "  30,\n",
       "  115,\n",
       "  37,\n",
       "  13],\n",
       " [57, 319, 101, 69],\n",
       " [107, 18630, 29, 25, 117, 910, 142, 330, 910, 13],\n",
       " [9, 9288, 768, 82, 37, 13],\n",
       " [94, 2583],\n",
       " [149, 33, 218, 13],\n",
       " [1575, 4919, 69],\n",
       " [340, 948, 1363, 82, 13],\n",
       " [1284, 8304, 367, 154, 37, 13],\n",
       " [37, 13],\n",
       " [40, 4095, 144, 395, 13],\n",
       " [243, 310, 704, 3049, 13],\n",
       " [539, 339, 101, 2738, 30, 69],\n",
       " [392, 1, 13],\n",
       " [27, 13],\n",
       " [60, 964, 1606, 116, 11037, 37, 13],\n",
       " [1, 764, 9, 2602, 137, 1211, 122, 305, 37, 13],\n",
       " [65, 870, 543, 37, 13],\n",
       " [1967, 372, 982, 1967, 77, 56, 76, 919, 37, 13]]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0.,\n",
       "       0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_tweets = pad_sequences(sequences_user, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "user_tweets_labels_predicted = model.predict(user_tweets)\n",
    "user_tweets_labels = np.round(user_tweets_labels_predicted.flatten())\n",
    "user_tweets_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n"
     ]
    }
   ],
   "source": [
    "numberOfTweets = len(user_tweets)\n",
    "print(numberOfTweets)\n",
    "numberOfDepressiveTweets = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tw_lab in user_tweets_labels:\n",
    "    if(tw_lab>0):\n",
    "        numberOfDepressiveTweets = numberOfDepressiveTweets+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "print(numberOfDepressiveTweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "objects = ('Negative', 'Positive')\n",
    "y_pos = np.arange(len(objects))\n",
    "Results = [numberOfDepressiveTweets,numberOfTweets-numberOfDepressiveTweets]\n",
    " \n",
    "plt.bar(y_pos, Results, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.ylabel('Number Of tweets')\n",
    "plt.title('Depressive Tweets Detection')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
